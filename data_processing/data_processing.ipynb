{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">A notebook to preprocess data: import from CSV, rename columns, do grouping, enrich the dataset (e.g., add tags), and finally save it as a CSV for later analysis. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Warning:</b> Be careful with the grouping granularity of the output CSV that is more high level than the actual granularity of the initial dataset (e.g., AC type, OPE_AL...). </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>TO DO:</b> old part / data error code</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from polars import col as d\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/sara/Desktop/ATSLab/data_processing/OneDrive_2025-07-22/Schedule_Data/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = os.path.join(folder_path, 'schedules_processed_carrier_hours_*_incsmall_novia_allairports.csv')\n",
    "\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "list_of_df = []\n",
    "\n",
    "for file_path in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    year = int(filename.split('_')[4])\n",
    "    print(year)\n",
    "    df = (\n",
    "        pl.scan_csv(file_path)\n",
    "        .with_columns(pl.lit(year).alias('YEAR'))\n",
    "        .with_columns(\n",
    "            Departures = d.Departures.cast(pl.Float64),\n",
    "            Arrivals = d.Arrivals.cast(pl.Float64),\n",
    "            DepartureSeats = d.DepartureSeats.cast(pl.Float64),\n",
    "            ArrivalSeats = d.ArrivalSeats.cast(pl.Float64),\n",
    "            DepartureFlightHours = d.DepartureFlightHours.cast(pl.Float64),\n",
    "            ArrivalFlightHours = d.ArrivalFlightHours.cast(pl.Float64),\n",
    "            EquipSACode = d.EquipSACode.cast(pl.Int64),\n",
    "            OriginAirport = d.OriginAirport.cast(pl.Utf8),\n",
    "            DestinationAirport = d.DestinationAirport.cast(pl.Utf8),\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "    if year == 2019:\n",
    "        df = df.drop(\"EquipATIBin\")\n",
    "        print('drop EquipATIBin column for 2019')\n",
    "\n",
    "    list_of_df.append(df)\n",
    "    \n",
    "print(\"Import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define path\n",
    "airports_metrics_path = os.path.join(folder_path, 'airports_v8_updatedmetrics_psos.csv')\n",
    "airports_lookup_path = os.path.join(folder_path, 'all_airports_lookup.csv')\n",
    "cities_metrics_path = os.path.join(folder_path, 'cities_v8_updatedmetrics_psos.csv')\n",
    "fleet_lookup_path = os.path.join(folder_path, 'fleet_lookups.csv')\n",
    "\n",
    "## define schema dict\n",
    "schema_dict_cities_metrics = {\n",
    "    \"Pop10\": pl.Float64,\n",
    "    \"Pop15\": pl.Float64\n",
    "}\n",
    "\n",
    "schema_dict_airports_metrics = {\n",
    "    \"Elev_ft\": pl.Float64,\n",
    "}\n",
    "\n",
    "## scan csv\n",
    "df_airports_lookup = pl.scan_csv(airports_lookup_path, encoding='utf8-lossy', null_values=[\"NA\"]).collect()\n",
    "df_cities_metrics = pl.scan_csv(cities_metrics_path, encoding='utf8-lossy', null_values=[\"NA\"], schema_overrides=schema_dict_cities_metrics).collect()\n",
    "df_airports_metrics = pl.scan_csv(airports_metrics_path, encoding='utf8-lossy', null_values=[\"NA\"], schema_overrides=schema_dict_airports_metrics).collect()\n",
    "df_fleet_lookup = pl.scan_csv(fleet_lookup_path, encoding='utf8-lossy', null_values=[\"NA\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_mapping_path = os.path.join(folder_path, 'subsidiariesGlobal_2019.csv')\n",
    "df_airline_mapping = pl.scan_csv(airline_mapping_path, encoding='utf8-lossy', null_values=[\"NA\"]).collect().rename({'Group':'AL_GROUP', 'Subsidiary':'OPE_AL'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_final_df = (\n",
    "    pl.concat(list_of_df)\n",
    "    .rename({'OriginAirport':'APT_CODE_A',\n",
    "    'DestinationAirport':'APT_CODE_B',\n",
    "    'TimeBin':'TIME_BIN',\n",
    "    'Equip': 'AC_TYPE',\n",
    "    'Carrier':'OPE_AL',\n",
    "    'EquipSACode': 'AIM_SIZE_CAT',\n",
    "    'Departures':'FLT_DEP',\n",
    "    'Arrivals':'FLT_ARR',\n",
    "    'DepartureSeats':'SEATS_DEP',\n",
    "    'ArrivalSeats':'SEATS_ARR',\n",
    "    'DepartureFlightHours':'FLT_HOURS_DEP',\n",
    "    'ArrivalFlightHours':'FLT_HOURS_ARR'\n",
    "    })\n",
    "\n",
    "    [['YEAR',\n",
    "    'APT_CODE_A',\n",
    "    'APT_CODE_B',\n",
    "    'TIME_BIN',\n",
    "    'AC_TYPE',\n",
    "    'OPE_AL',\n",
    "    'AIM_SIZE_CAT',\n",
    "    'FLT_DEP',\n",
    "    'FLT_ARR',\n",
    "    'SEATS_DEP',\n",
    "    'SEATS_ARR',\n",
    "    'FLT_HOURS_DEP',\n",
    "    'FLT_HOURS_ARR']]\n",
    ")\n",
    "\n",
    "print(\"Shape of the final schedule df :\", schedules_final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_airports_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airports_lookup_modif = (\n",
    "    df_airports_lookup\n",
    "    .rename({ ## not sure about the APT prefix\n",
    "        'Number':'APT_ID',\n",
    "        'Airport':'APT_CODE',\n",
    "        'Name':'APT_NAME',\n",
    "        'City':'APT_CITY_NAME',\n",
    "        'CityName': 'APT_CITY_FULL_NAME',\n",
    "        'Country': 'APT_COUNTRY_CODE',\n",
    "        'CountryName':'APT_COUNTRY_NAME',\n",
    "        'Region':'APT_REGION', ## _29 \n",
    "        'Latitude':'LATITUDE',\n",
    "        'Longitude':'LONGITUDE'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_cities_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities_metrics_modif = (\n",
    "    df_cities_metrics\n",
    "    .rename({\n",
    "        'Number':'METRO_ID',\n",
    " 'City':'METRO_CITY',\n",
    " 'Country': 'METRO_COUNTRY',\n",
    " 'N_airports_2015':'NB_APT_2015',\n",
    " 'Mid_Long':'METRO_AVG_LONG',\n",
    " 'Mid_Lat':'METRO_AVG_LAT',\n",
    " 'Pop10':'POPU_2010',\n",
    " 'Pop11':'POPU_2011',\n",
    " 'Pop12':'POPU_2012',\n",
    " 'Pop13':'POPU_2013',\n",
    " 'Pop14':'POPU_2014',\n",
    " 'Pop15':'POPU_2015',\n",
    " 'Inc10_LocalCurrency':'INC_2010_LC',\n",
    " 'Inc11_LocalCurrency':'INC_2011_LC',\n",
    " 'Inc12_LocalCurrency':'INC_2012_LC',\n",
    " 'Inc13_LocalCurrency':'INC_2013_LC',\n",
    " 'Inc14_LocalCurrency':'INC_2014_LC',\n",
    " 'Inc15_LocalCurrency':'INC_2015_LC',\n",
    " 'Inc10_USD2015_MER':'INC_2010_USD15',\n",
    " 'Inc11_USD2015_MER':'INC_2011_USD15',\n",
    " 'Inc12_USD2015_MER':'INC_2012_USD15',\n",
    " 'Inc13_USD2015_MER':'INC_2013_USD15',\n",
    " 'Inc14_USD2015_MER':'INC_2014_USD15',\n",
    " 'Inc15_USD2015_MER':'INC_2015_USD15',\n",
    " 'Spec':'IS_SPECIAL',\n",
    " 'Access_Road':'HAS_ACCESS_ROAD',\n",
    " 'Access_Rail':'HAS_ACCESS_RAIL',\n",
    " 'Access_Sea':'HAS_ACCESS_SEA',\n",
    " 'A1':'APT_ID_1',\n",
    " 'A2':'APT_ID_2',\n",
    " 'A3':'APT_ID_3',\n",
    " 'A4':'APT_ID_4',\n",
    " 'A5':'APT_ID_5',\n",
    " 'A6':'APT_ID_6',\n",
    " 'A7':'APT_ID_7',\n",
    " 'A8':'APT_ID_8',\n",
    " 'A9':'APT_ID_9',\n",
    " 'A10':'APT_ID_10',\n",
    " 'A11':'APT_ID_11',\n",
    " 'A12':'APT_ID_12',\n",
    " 'A13':'APT_ID_13',\n",
    " 'C1':'CITY_CODE_1',\n",
    " 'C2':'CITY_CODE_2',\n",
    " 'C3':'CITY_CODE_3',\n",
    " 'C4':'CITY_CODE_4',\n",
    " 'C5':'CITY_CODE_5',\n",
    " 'C6':'CITY_CODE_6',\n",
    " 'C7':'CITY_CODE_7',\n",
    " 'C8':'CITY_CODE_8',\n",
    " 'C9':'CITY_CODE_9',\n",
    " 'C10':'CITY_CODE_10',\n",
    " 'C11':'CITY_CODE_11',\n",
    " 'C12':'CITY_CODE_12',\n",
    " 'C13':'CITY_CODE_13',\n",
    "\n",
    " 'ICAO.Region.Code':'REGION_ID',\n",
    " 'Country.Code':'COUNTRY_ID',\n",
    " 'Optional.Region.Code':'OPT_REGION_ID',\n",
    " 'N_Runways_2015':'NB_RUNWAYS_2015',\n",
    " 'Capital_city':'IS_CAPITAL',\n",
    " 'nom_single':'MAIN_CITY_NAME',\n",
    " 'global_hub':'IS_GLOBAL_HUB',\n",
    " 'domestic_hub':'IS_DOMESTIC_HUB',\n",
    " 'GaWC_index':'GAWC_INDEX',\n",
    "\n",
    " 'Deps_2015':'FLT_DEP_2015',\n",
    " 'Seats_2015':'SEATS_DEP_2015',\n",
    " 'Links_to_other_cities_2015':'LINKS_TO_CITY_2015',\n",
    " 'Domestic_links_to_other_cities_2015':'DOMESTIC_LINKS_TO_CITY_2015',\n",
    " 'Daily_links_to_other_cities_2015':'DAILY_LINKS_TO_CITY_2015',\n",
    " 'Links_to_other_cities_over_1000mi_2015':'LINKS_TO_CITY_OV_1000MI_2015',\n",
    " 'Links_to_other_cities_over_3000mi_2015':'LINKS_TO_CITY_OV_3000MI_2015',\n",
    " 'In_set_proportion_of_global_RPK_2015':'RATIO_GLOBAL_RPK_2015',\n",
    "\n",
    " 'Deps_2019':'FLT_DEP_2019',\n",
    " 'Seats_2019':'SEATS_DEP_2019',\n",
    " 'Links_to_other_cities_2019':'LINKS_TO_CITY_2019',\n",
    " 'Domestic_links_to_other_cities_2019':'DOMESTIC_LINKS_TO_CITY_2019',\n",
    " 'Daily_links_to_other_cities_2019':'DAILY_LINKS_TO_CITY_2019',\n",
    " 'Links_to_other_cities_over_1000mi_2019':'LINKS_TO_CITY_OV_1000MI_2019',\n",
    " 'Links_to_other_cities_over_3000mi_2019':'LINKS_TO_CITY_OV_3000MI_2019',\n",
    " 'In_set_proportion_of_global_RPK_2019':'RATIO_GLOBAL_RPK_2019',\n",
    "\n",
    " 'Deps_2023':'FLT_DEP_2023',\n",
    " 'Seats_2023':'SEATS_DEP_2023',\n",
    " 'Links_to_other_cities_2023':'LINKS_TO_CITY_2023',\n",
    " 'Domestic_links_to_other_cities_2023':'DOMESTIC_LINKS_TO_CITY_2023',\n",
    " 'Daily_links_to_other_cities_2023':'DAILY_LINKS_TO_CITY_2023',\n",
    " 'Links_to_other_cities_over_1000mi_2023':'LINKS_TO_CITY_OV_1000MI_2023',\n",
    " 'Links_to_other_cities_over_3000mi_2023':'LINKS_TO_CITY_OV_3000MI_2023',\n",
    " 'In_set_proportion_of_global_RPK_2023':'RATIO_GLOBAL_RPK_2023'\n",
    "\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_airports_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airports_metrics_modif = (\n",
    "    df_airports_metrics\n",
    "    .rename({\n",
    "        'Number':'OLD_ID',\n",
    "        'Number_2005':'ID_2005',\n",
    "        'Code':'APT_CODE',\n",
    "        'ICAO.Code':'APT_ICAO_CODE',\n",
    "        'Name':'APT_CITY_LONG_NAME',\n",
    "        'Country':'APT_COUNTRY_LONG_NAME',\n",
    "        'Country.Code':'APT_COUNTRY_CODE',\n",
    "        'Latitude':'LATITUDE',\n",
    "        'Longitude':'LONGITUDE',\n",
    "        'Timezone..19.Apr.2016.':'TIME_ZONE_2016',\n",
    "        'World.Region':'REGION_ID',\n",
    "        'City': 'APT_CITY_FULL_NAME_LOWER',\n",
    "        'Island':'IS_ISLAND',\n",
    "        'Elev_ft':'ELEV_FT',\n",
    "        'Nrunways':'NB_RUNWAYS',\n",
    "        'Longest_runway_ft':'LONGEST_RUNWAY_FT',\n",
    "        'opened_year':'OPENING_YEAR',\n",
    "        'build_year':'BUILDING_YEAR',\n",
    "        'closed_year':'CLOSING_YEAR',\n",
    "        'wikipedia_reference':'WIKI_LINK',\n",
    "        'airport_city_greatcircle_dist_km': 'APT_CITY_GC_DIST_KM',\n",
    "        'airport_city_drive_dist_km':'APT_CITY_DRIVE_DIST_KM',\n",
    "        'airport_city_drive_time_h':'APT_CITY_DRIVE_TIME_H',\n",
    "\n",
    "        'Scheduled_departures_2015': 'FLT_SCHEDULED_DEP_2015',\n",
    "        'Scheduled_seats_2015':'SEATS_SCHEDULED_DEP_2015', ## is it DEP SEATS ? or DEP & ARR SEATS ? \n",
    "        'Cumulative_scheduled_seats_2015':'CUMU_SEATS_SCHEDULED_DEP_2015',\n",
    "        'Cumulative_scheduled_departures_2015':'CUMU_FLT_SCHEDULED_DEP_2015',\n",
    "\n",
    "        'Scheduled_departures_2019': 'FLT_SCHEDULED_DEP_2019',\n",
    "        'Scheduled_seats_2019':'SEATS_SCHEDULED_DEP_2019', ## is it DEP SEATS ? or DEP & ARR SEATS ? \n",
    "        'Cumulative_scheduled_seats_2019':'CUMU_SEATS_SCHEDULED_DEP_2019',\n",
    "        'Cumulative_scheduled_departures_2019':'CUMU_FLT_SCHEDULED_DEP_2019',\n",
    "\n",
    "        'Scheduled_departures_2023': 'FLT_SCHEDULED_DEP_2023',\n",
    "        'Scheduled_seats_2023':'SEATS_SCHEDULED_DEP_2023', ## is it DEP SEATS ? or DEP & ARR SEATS ? \n",
    "        'Cumulative_scheduled_seats_2023':'CUMU_SEATS_SCHEDULED_DEP_2023',\n",
    "        'Cumulative_scheduled_departures_2023':'CUMU_FLT_SCHEDULED_DEP_2023',\n",
    "\n",
    "        'Departing_leg_passengers_2015':'LEG_PAX_DEP_2015',\n",
    "        'Departing_leg_passengers_2019':'LEG_PAX_DEP_2019', \n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_fleet_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fleet_lookup_modif = (\n",
    "    df_fleet_lookup\n",
    "    .rename({\n",
    "        'Name':'AC_FULL_NAME',\n",
    "        'ICAOcode':'AC_ICAO_CODE',\n",
    "        'IATAcode':'AC_IATA_CODE',\n",
    "        'estSeats':'SEATS_ESTIM',\n",
    "        'Manufacturer_Country':'MANUF_COUNTRY_NAME',\n",
    "        'Military.Utility':'IS_MILITARY_UTIL',\n",
    "        'AIM_SizeCat':'AIM_SIZE_CAT',\n",
    "        'ATI_SizeCat':'ATI_SIZE_CAT',\n",
    "        'TypicalSeats':'SEATS_TYPICAL',\n",
    "        'BuiltFrom':'BUILT_FROM',\n",
    "        'BuiltTo':'BUILT_TO',\n",
    "        'MaxFreightVolFreighterm3':'MAX_VOL_FREIGHTER_M3',\n",
    "        'MaxFreightWeightFreighterTonne':'MAX_WEIGHT_FREIGHTER_T',\n",
    "        'btscode':'BTS_CODE',\n",
    "        'FlightGlobalname':'AC_GLOBAL_NAME',\n",
    "        'NewFlightGlobalName':'AC_NEW_GLOBAL_NAME',\n",
    "        'exFleetTotalP':'FLEET_TOTAL_P',\n",
    "        'exFleetTotalF':'FLEET_TOTAL_F',\n",
    "        'AvgSeatsFG':'AVG_SEATS_FG', ## what is FG ?\n",
    "        'AvgMaxPayloadP':'AVG_MAX_PAYLOAD_P',\n",
    "        'AvgMaxPayloadF':'AVG_MAX_PAYLOAD_F',\n",
    "        'AvgMaxCargoPayloadP':'AVG_MAX_CARGO_PAYLOAD_P',\n",
    "        'EngineType':'ENGINE_TYPE'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_sequences(years):\n",
    "    years = sorted(years)\n",
    "    groups = [[years[0]]]\n",
    "    for y in years[1:]:\n",
    "        if y == groups[-1][-1] + 1:\n",
    "            groups[-1].append(y)\n",
    "        else:\n",
    "            groups.append([y])\n",
    "    return groups\n",
    "\n",
    "\n",
    "def find_sequence_length(year: int, sequences: list[list[int]]) -> int:\n",
    "    for seq in sequences:\n",
    "        if year in seq:\n",
    "            return len(seq)\n",
    "    return 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced schedules dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced = (\n",
    "    schedules_final_df\n",
    "\n",
    "    .filter(d.APT_CODE_A != d.APT_CODE_B) ## filter out when the airport A and airport B are the same\n",
    "\n",
    "    ## group by\n",
    "    .group_by(['YEAR', 'APT_CODE_A', 'APT_CODE_B']) \n",
    "    .agg(d.FLT_DEP.sum().alias('FLT'), d.FLT_ARR.sum(), d.SEATS_DEP.sum().alias('SEATS'), d.SEATS_ARR.sum(), d.FLT_HOURS_DEP.sum().alias('FLT_HOURS'), d.FLT_HOURS_ARR.sum(), d.OPE_AL.unique().count().alias('NB_OPE_AL'), d.OPE_AL.unique().alias('LIST_OPE_AL'))\n",
    "\n",
    "    ## add the REGION ID\n",
    "    .join(df_airports_metrics_modif.select('APT_CODE', 'REGION_ID').rename({'APT_CODE':'APT_CODE_A', 'REGION_ID':'REGION_ID_A'}).unique(), how = 'left', on = ['APT_CODE_A'])\n",
    "    .join(df_airports_metrics_modif.select('APT_CODE', 'REGION_ID').rename({'APT_CODE':'APT_CODE_B', 'REGION_ID':'REGION_ID_B'}).unique(), how = 'left', on = ['APT_CODE_B'])\n",
    "\n",
    "    ## add the list of existing year of an airport pair\n",
    "    .pipe( ## over function don't work on my version that's weird\n",
    "        lambda df: df.join(\n",
    "            df.group_by([\"APT_CODE_A\", \"APT_CODE_B\"])\n",
    "              .agg(d.YEAR.unique().alias(\"LIST_EXISTING_YEAR\"), d.YEAR.unique().count().alias('NB_EXISTING_YEAR'), d.YEAR.max().alias('LAST_EXISTING_YEAR'), d.YEAR.min().alias('FIRST_EXISTING_YEAR')),\n",
    "            on=[\"APT_CODE_A\", \"APT_CODE_B\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## add tag to know when the route open, end, reopen, break\n",
    "    .with_columns(IS_OPENING = (d.FIRST_EXISTING_YEAR == d.YEAR) & (d.YEAR != 2000))\n",
    "    .with_columns(IS_END = (d.LAST_EXISTING_YEAR == d.YEAR) & (d.YEAR != 2023))\n",
    "    .with_columns(IS_REOPENING = (~d.LIST_EXISTING_YEAR.list.contains(d.YEAR - 1)) & (d.YEAR != d.FIRST_EXISTING_YEAR))\n",
    "    .with_columns(IS_PAUSE = (~d.LIST_EXISTING_YEAR.list.contains(d.YEAR + 1)) & (d.YEAR != d.LAST_EXISTING_YEAR))\n",
    "\n",
    "\n",
    "    ## apply previous function to have a list of list of the consecutive years the routes was open\n",
    "    .with_columns(LIST_CONSEC_YEAR = d.LIST_EXISTING_YEAR.map_elements(consecutive_sequences, return_dtype=pl.List(pl.List(pl.Int32))))\n",
    "    .with_columns([pl.struct([\"YEAR\", \"LIST_CONSEC_YEAR\"]).map_elements(lambda x: find_sequence_length(x[\"YEAR\"], x[\"LIST_CONSEC_YEAR\"]),return_dtype=pl.Int32).alias(\"CONSEC_YEAR_OPEN_DURATION\")])\n",
    "\n",
    "    ## how many breaks the route had, in total\n",
    "    .with_columns(TOTAL_BREAKS = d.LIST_CONSEC_YEAR.list.len() -1)\n",
    "\n",
    "    ## the duration of the first opening\n",
    "    .with_columns(DURATION_FIRST_OPENING = d.LIST_CONSEC_YEAR.list.first().list.len().cast(pl.Int32))\n",
    "\n",
    "    # ## the duration of the first break\n",
    "    # .pipe(\n",
    "    #     lambda df: df.join(\n",
    "    #                         df.filter(d.TOTAL_BREAKS != 0)\n",
    "    #                         .with_columns(DURATION_FIRST_BREAK = d.LIST_CONSEC_YEAR.list.get(1).list.first() - d.LIST_CONSEC_YEAR.list.first().list.last() - 1)\n",
    "    #                         .select(['YEAR', 'APT_CODE_A', 'APT_CODE_B', 'DURATION_FIRST_BREAK'])\n",
    "    #         ,\n",
    "    #         on=['YEAR', 'APT_CODE_A', 'APT_CODE_B',],\n",
    "    #         how=\"left\"\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "\n",
    "    ## add columns of 1 to sum when grouping in the future (i'm sure there is a smarter way to do it)\n",
    "    .with_columns(NB_OPENING_RTE = pl.when(d.IS_OPENING) ## no need to put a condition for the year it's already done before\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_SHORT_OPENING_RTE = pl.when(d.IS_OPENING & (d.DURATION_FIRST_OPENING <= 3))\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_LONG_OPENING_RTE = pl.when(d.IS_OPENING & (d.DURATION_FIRST_OPENING > 3))\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_ENDING_RTE = pl.when(d.IS_END) ## same here but for 2023\n",
    "                             .then(1)\n",
    "                             .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_REOPENING_RTE = pl.when(d.IS_REOPENING) ## same comment\n",
    "                                          .then(1)\n",
    "                                          .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_PAUSE_RTE = pl.when(d.IS_PAUSE) ## same comment\n",
    "                            .then(1)\n",
    "                            .otherwise(0)\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same but without covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_wo_covid = (\n",
    "    schedules_final_df\n",
    "\n",
    "    .filter(d.YEAR < 2020)\n",
    "    .filter(d.APT_CODE_A != d.APT_CODE_B) ## filter out when the airport A and airport B are the same\n",
    "\n",
    "    ## group by\n",
    "    .group_by(['YEAR', 'APT_CODE_A', 'APT_CODE_B']) \n",
    "    .agg(d.FLT_DEP.sum().alias('FLT'), d.FLT_ARR.sum(), d.SEATS_DEP.sum().alias('SEATS'), d.SEATS_ARR.sum(), d.FLT_HOURS_DEP.sum().alias('FLT_HOURS'), d.FLT_HOURS_ARR.sum(), d.OPE_AL.unique().count().alias('NB_OPE_AL'))\n",
    "\n",
    "    ## add the REGION ID\n",
    "    .join(df_airports_metrics_modif.select('APT_CODE', 'REGION_ID').rename({'APT_CODE':'APT_CODE_A', 'REGION_ID':'REGION_ID_A'}).unique(), how = 'left', on = ['APT_CODE_A'])\n",
    "    .join(df_airports_metrics_modif.select('APT_CODE', 'REGION_ID').rename({'APT_CODE':'APT_CODE_B', 'REGION_ID':'REGION_ID_B'}).unique(), how = 'left', on = ['APT_CODE_B'])\n",
    "\n",
    "    ## add the list of existing year of an airport pair\n",
    "    .pipe( ## over function don't work on my version that's weird\n",
    "        lambda df: df.join(\n",
    "            df.group_by([\"APT_CODE_A\", \"APT_CODE_B\"])\n",
    "              .agg(d.YEAR.unique().alias(\"LIST_EXISTING_YEAR\"), d.YEAR.unique().count().alias('NB_EXISTING_YEAR'), d.YEAR.max().alias('LAST_EXISTING_YEAR'), d.YEAR.min().alias('FIRST_EXISTING_YEAR')),\n",
    "            on=[\"APT_CODE_A\", \"APT_CODE_B\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## add tag to know when the route open, end, reopen, break\n",
    "    .with_columns(IS_OPENING = (d.FIRST_EXISTING_YEAR == d.YEAR) & (d.YEAR != 2000))\n",
    "    .with_columns(IS_END = (d.LAST_EXISTING_YEAR == d.YEAR) & (d.YEAR != 2023))\n",
    "    .with_columns(IS_REOPENING = (~d.LIST_EXISTING_YEAR.list.contains(d.YEAR - 1)) & (d.YEAR != d.FIRST_EXISTING_YEAR))\n",
    "    .with_columns(IS_PAUSE = (~d.LIST_EXISTING_YEAR.list.contains(d.YEAR + 1)) & (d.YEAR != d.LAST_EXISTING_YEAR))\n",
    "\n",
    "\n",
    "    ## apply previous function to have a list of list of the consecutive years the routes was open\n",
    "    .with_columns(LIST_CONSEC_YEAR = d.LIST_EXISTING_YEAR.map_elements(consecutive_sequences, return_dtype=pl.List(pl.List(pl.Int32))))\n",
    "    .with_columns([pl.struct([\"YEAR\", \"LIST_CONSEC_YEAR\"]).map_elements(lambda x: find_sequence_length(x[\"YEAR\"], x[\"LIST_CONSEC_YEAR\"]),return_dtype=pl.Int32).alias(\"CONSEC_YEAR_OPEN_DURATION\")])\n",
    "\n",
    "    ## how many breaks the route had, in total\n",
    "    .with_columns(TOTAL_BREAKS = d.LIST_CONSEC_YEAR.list.len() -1)\n",
    "\n",
    "    ## the duration of the first opening\n",
    "    .with_columns(DURATION_FIRST_OPENING = d.LIST_CONSEC_YEAR.list.first().list.len().cast(pl.Int32))   \n",
    "\n",
    "    ## add columns of 1 to sum when grouping in the future (i'm sure there is a smarter way to do it)\n",
    "    .with_columns(NB_OPENING_RTE = pl.when(d.IS_OPENING) ## no need to put a condition for the year it's already done before\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_SHORT_OPENING_RTE = pl.when(d.IS_OPENING & (d.DURATION_FIRST_OPENING <= 3))\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_LONG_OPENING_RTE = pl.when(d.IS_OPENING & (d.DURATION_FIRST_OPENING > 3))\n",
    "                                     .then(1)\n",
    "                                     .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_ENDING_RTE = pl.when(d.IS_END) ## same here but for 2023\n",
    "                             .then(1)\n",
    "                             .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_REOPENING_RTE = pl.when(d.IS_REOPENING) ## same comment\n",
    "                                          .then(1)\n",
    "                                          .otherwise(0)\n",
    "    )\n",
    "\n",
    "    .with_columns(NB_PAUSE_RTE = pl.when(d.IS_PAUSE) ## same comment\n",
    "                            .then(1)\n",
    "                            .otherwise(0)\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Transatlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_filtered = (\n",
    "    df_enhanced\n",
    "    .filter(d.REGION_ID_A.is_in([10,13]))\n",
    "    .filter(d.REGION_ID_B.is_in([10,13]))\n",
    "\n",
    "    ## add tag for futur plot\n",
    "    .with_columns(MKT_TYPE = pl.when((d.REGION_ID_A == d.REGION_ID_B) & (d.REGION_ID_A == 10))\n",
    "                               .then(pl.lit('INTRA_US'))\n",
    "                               .when((d.REGION_ID_A == d.REGION_ID_B) & (d.REGION_ID_A == 13))\n",
    "                               .then(pl.lit('INTRA_EUR'))\n",
    "                               .otherwise(pl.lit('INTER'))\n",
    "    )\n",
    "\n",
    "    ## the direction (to be more precise on the INTER)\n",
    "    .with_columns(DIRECTION = pl.when((d.REGION_ID_A == 10) & (d.REGION_ID_B == 13))\n",
    "                                .then(pl.lit('US_to_EUR'))\n",
    "                                .when((d.REGION_ID_A == 13) & (d.REGION_ID_B == 10))\n",
    "                                .then(pl.lit('EUR_to_US'))\n",
    "                                .otherwise(d.MKT_TYPE)\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_wo_covid_filtered = (\n",
    "    df_enhanced_wo_covid\n",
    "    .filter(d.REGION_ID_A.is_in([10,13]))\n",
    "    .filter(d.REGION_ID_B.is_in([10,13]))\n",
    "\n",
    "    ## add tag for futur plot\n",
    "    .with_columns(MKT_TYPE = pl.when((d.REGION_ID_A == d.REGION_ID_B) & (d.REGION_ID_A == 10))\n",
    "                               .then(pl.lit('INTRA_US'))\n",
    "                               .when((d.REGION_ID_A == d.REGION_ID_B) & (d.REGION_ID_A == 13))\n",
    "                               .then(pl.lit('INTRA_EUR'))\n",
    "                               .otherwise(pl.lit('INTER'))\n",
    "    )\n",
    "\n",
    "    ## the direction (to be more precise on the INTER)\n",
    "    .with_columns(DIRECTION = pl.when((d.REGION_ID_A == 10) & (d.REGION_ID_B == 13))\n",
    "                                .then(pl.lit('US_to_EUR'))\n",
    "                                .when((d.REGION_ID_A == 13) & (d.REGION_ID_B == 10))\n",
    "                                .then(pl.lit('EUR_to_US'))\n",
    "                                .otherwise(d.MKT_TYPE)\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scheduled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transtlantic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_filtered.write_parquet(\"scheduled_dataset_transatlantic_enhanced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_wo_covid_filtered.write_parquet(\"scheduled_dataset_wo_covid_transatlantic_enhanced.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fleet_lookup_modif.write_csv(\"df_fleet_lookup_modif.csv\")\n",
    "df_airports_lookup_modif.write_csv(\"df_airports_lookup_modif.csv\")\n",
    "df_airports_metrics_modif.write_csv(\"df_airports_metrics_modif.csv\")\n",
    "df_cities_metrics_modif.write_csv(\"df_cities_metrics_modif.csv\")\n",
    "df_airline_mapping.write_csv(\"df_airline_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD - data error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes apt A == apt B\n",
    "- seats dep != seats arr --> normal but also not normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_error_look = (\n",
    "#     df_filtered\n",
    "#     .filter(d.APT_CODE_A != d.APT_CODE_B)\n",
    "\n",
    "#     .group_by(['YEAR', 'MKT_TYPE'])\n",
    "#     .agg(d.SEATS_DEP.sum(), d.SEATS_ARR.sum())\n",
    "#     .with_columns(DIFF = d.SEATS_DEP - d.SEATS_ARR)\n",
    "#     # .select('SEATS_DEP', 'SEATS_ARR')\n",
    "#     # .sum()\n",
    "#     .sort('YEAR', 'MKT_TYPE')\n",
    "#     .to_pandas()\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     px.line(df_error_look,\n",
    "#     x = 'YEAR', y = ['SEATS_DEP', 'SEATS_ARR'],\n",
    "#     facet_col = 'MKT_TYPE'\n",
    "#     )\n",
    "#     .update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     px.line(df_error_look,\n",
    "#     x = 'YEAR', y = 'DIFF',\n",
    "#     facet_col = 'MKT_TYPE'\n",
    "#     )\n",
    "#     .update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_check_filter = (\n",
    "#     df_filtered\n",
    "#     .filter(d.APT_CODE_A == d.APT_CODE_B)\n",
    "#     .group_by(['YEAR', 'MKT_TYPE'])\n",
    "#     .agg(d.SEATS_DEP.sum().alias('SEATS'))\n",
    "#     .sort('YEAR')\n",
    "#     .to_pandas()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     px.line(graph_check_filter,\n",
    "#     x = 'YEAR', y = 'SEATS',\n",
    "#     markers = True, \n",
    "#     facet_col = 'MKT_TYPE'\n",
    "#     )\n",
    "#     .update_yaxes(matches=None, showticklabels=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     df_filtered\n",
    "#     .filter(d.APT_CODE_A != d.APT_CODE_B)\n",
    "#     .group_by(['YEAR', 'APT_CODE_A', 'APT_CODE_B', 'REGION_ID_A', 'REGION_ID_B', 'MKT_TYPE'])\n",
    "#     .agg(d.FLT_DEP.sum(), d.FLT_ARR.sum(), d.SEATS_DEP.sum(), d.SEATS_ARR.sum())\n",
    "\n",
    "#     .filter(d.SEATS_DEP != d.SEATS_ARR)\n",
    "\n",
    "#     .with_columns(IS_MORE_THAN_10_FLT_DIFF=abs(d.FLT_DEP - d.FLT_ARR)>10)\n",
    "\n",
    "#     .group_by(['YEAR', 'MKT_TYPE', 'IS_MORE_THAN_10_FLT_DIFF'])\n",
    "#     .agg(d.SEATS_DEP.sum(), d.SEATS_ARR.sum())\n",
    "\n",
    "#     .sort('YEAR', 'MKT_TYPE')\n",
    "    \n",
    "\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
